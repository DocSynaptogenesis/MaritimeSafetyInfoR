---
title: "PDF Inferno"
author: "Brendan Knapp"
date: "November 30, 2017"
output: 
  github_document:
    toc: true
---

<!-- <base target="_blank"/> -->

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = TRUE)
```

The data obtained in this code is available on GitHub [here](https://github.com/DocSynaptogenesis/MaritimeSafetyInfoR).

# Notice to Mariners

* The U.S. Notice to Mariners data is available in multiple formats, but they all leave something to be desired in terms of data quality and completeness. 

* Unfortunately, there are many Broadcast Warnings included in the PDF publications that are missing in other available formats, making the PDF versions the most complete source of data.

> __To overcome this, we must step into the seventh circle of data mining hell: poorly structured PDF files.__

# Dependencies

```{r}
library(dplyr)
library(tidyr)
library(stringr)
library(rvest)
library(pdftools)
```

# Most Recently Published Issue

We can tap into the NGA site's HTML and scrape the latest publication's issue number like so:

```{r}
notice_to_mariners_url <- "https://msi.nga.mil/NGAPortal/MSI.portal?_nfpb=true&_st=&_pageLabel=msi_portal_page_61"

most_recent_issue <- notice_to_mariners_url %>%
  read_html() %>%
  html_nodes("tr:nth-child(2) .dec-inv") %>%
  html_text()

paste(cat("The most recent issue is:\n"), most_recent_issue)

most_recent_issue %<>%
  str_extract("\\d{2}") %>%
  as.numeric

paste(cat("The week number we will use for our URL range is:\n"), most_recent_issue)
```

For demonstration, you can see the 
* table of contents of the most recent Notice to Mariners (48/2017)
    + [here](https://msi.nga.mil/NGAPortal/MSI.portal?_nfpb=true&_st=&_pageLabel=msi_ntm_pubview_page&CCD_itemID=201748) 
* the Broadcast Warnings/MARAD Advisories/Special Warnings PDF
    + [here](https://msi.nga.mil/MSISiteContent/StaticFiles/NAV_PUBS/UNTM/201748/Broadcast_Warn.pdf.)

# URLs

The URLs for 2017's PDFs follow a template containing the four digit year followed by the two digit week.

Here we replace the two digit week with `%s` so that we can format a the template for each week of the year.

```{r}
url_template <-
  "https://msi.nga.mil/MSISiteContent/StaticFiles/NAV_PUBS/UNTM/201701/Broadcast_Warn.pdf" %>%
  str_replace("(\\d{4})\\d{2}", "\\1%s")

paste(cat("Our formatted URL template is:\n"), url_template)
```

Using `sprintf()` with `url_template`, we can build a `vector` of `urls` with each two digit week up until `most_recent_issue`.

```{r}
urls <- sprintf(url_template,
                str_pad(seq(1:most_recent_issue),
                        width = 2,
                        pad = "0"))

paste(cat("Our URLs follow this pattern:\n"), urls[1:5])
```

We will eventually iterate through `urls` to extract and parse the desired information.

# Setup for Data Extraction

## Useful Regular Expressions

Let's prepare a handy regex variable of months in the format that we see is used for dates in the PDF documents.

```{r}
months_regex <- c("JAN", "FEB", "MAR", "APR", "MAY", "JUN",
                  "JUL", "AUG", "SEP", "OCT", "NOV", "DEC") %>%
  str_c(collapse = "|")

paste(cat("The resulting regex looks like so:\n"), months_regex)
```

We'll also prepare a regex variable containing terms of interest to extract from the messages to facilitate filtering for research.

```{r}
relevant_terms_regex <- c("ROCKET", "MISSILE", "HAZARDOUS",
                          "GUNNERY", "LAUNCHING") %>%
  str_c(collapse = "|")

paste(cat("The resulting regex looks like so:\n"), relevant_terms_regex)
```

## `list_to_text()` Modification

This is an incredibly handy `list_to_text()` function that is slightly modified from the that in the [`exploratory` package](https://github.com/exploratory-io/exploratory_func).

The difference is that the `character` returned contains only `unique()` elements from the original `list`, which I have found to be useful when you have a set of key terms with which you'd like to annotate a text dataset.

```{r}
list_to_text <- function(column, sep = ":"){
  loadNamespace("stringr")
  ret <- sapply(column, function(x){
    ret <- stringr::str_c(unique(x), collapse = sep)
    if(identical(ret, character(0))){
      # if it's character(0)
      NA
    } else {
      ret}})
  as.character(ret)
}
```

## Empty `tibble` data frame

We'll create an empty `tibble` which we will use to store our information.

```{r}
raw_df <- tibble()
```

# Data Wrangling
## Extraction and Initial Cleaning

* pull the PDF text: `pdf_text()`
* remove document's header and footer clutter: `str_replace_all()`
* annotate the positions to 
    + split desired text from body clutter into a nested `list` : `str_replace_all()`
    + split desired entries into separate fields : `str_replace_all()`
* unlist top layer: `unlist()`
* keep only those  elements in the `list` with our annotations: `str_subset()`
* remove inner excess white space: `str_replace_all()`
* remove outer excess white space: `str_trim()`
* convert the `list` to a data frame: `as_tibble()`
* bind data to `raw_df`: `bind_rows()`
* rinse, recycle, repeat

```{r}
for(URL in urls){
  temp_df <- URL %>%
    pdf_text() %>%
    str_replace_all("([I]+-\\d\\.\\d\\s)|(NM\\s\\d+/\\d+)|(SECTION\\s[I]+)", "") %>%
    str_replace_all("(\\d+/\\d+\\(.*?\\)\\.)", "~_~~\\1") %>% 
    str_split("~_") %>%
    unlist(., recursive = FALSE) %>%
    str_subset("~~") %>%
    str_replace_all("~~", "") %>%
    str_replace_all("\\s+", " ") %>%
    str_trim %>%
    as_tibble
    
  raw_df <- bind_rows(raw_df, temp_df)
}
```

## The Heavy Lifting

> There are more steps than required here as the format for which that will be used is not yet determined. Modification from this point into whatever structure is required for future analysis will be simple.

```{r message=FALSE, warning=FALSE}
total_df <- raw_df %>%
  mutate(value = str_replace(value, "(\\)\\.)", "\\)\\.~")) %>%
    separate(value, c("ID", "message"), sep =  "~") %>%
    mutate(coords_1 = str_extract_all(message,
                                      "\\d+-\\d+\\.\\d+[NS]{1}\\s\\d+-\\d+\\.\\d+[EW]"),
           coords_2 = str_extract_all(message,
                                      "\\d+-\\d+[NS]\\s\\d+-\\d+[EW]")) %>%
    mutate(coords_1 = list_to_text(coords_1),
           coords_2 = list_to_text(coords_2)) %>%
    separate(coords_1, paste0("fine_", c(1:40)), sep = ":") %>%
    separate(coords_2, paste0("coarse_", c(1:40)), sep = ":") %>%
    gather(coord_precision, coords, contains("_")) %>%
    drop_na(coords) %>%
    separate(coords, c("lat", "long"), sep = " ") %>%
    mutate(lat = str_replace(lat, "-", "d"),
           lat = str_replace(lat, "-", "'"),
           lat = str_replace(lat, "\\.", "'"),
           lat = if_else(str_detect(coord_precision, "fine"),
                         str_replace_all(lat, str_sub(lat, -1L, -1L),
                                         paste0("\\\" ", str_sub(lat, -1L, -1L))),
                         str_replace_all(lat, str_sub(lat, -1L, -1L),
                                         paste0(" ", str_sub(lat, -1L, -1L)))),
           lat = as.numeric(sp::char2dms(lat))) %>%
    mutate(long = str_replace(long, "-", "d"),
           long = str_replace(long, "-", "'"),
           long = str_replace(long, "\\.", "'"),
           long = if_else(str_detect(coord_precision, "fine"),
                          str_replace_all(long, str_sub(long, -1L, -1L),
                                          paste0("\\\" ", str_sub(long, -1L, -1L))),
                          str_replace_all(long, str_sub(long, -1L, -1L),
                                          paste0(" ", str_sub(long, -1L, -1L)))),
           long = as.numeric(sp::char2dms(long))) %>%
    mutate(relevant_terms = str_extract_all(message, relevant_terms_regex),
           relevant_terms = list_to_text(relevant_terms)) %>%
    mutate(time_date = str_extract(message, "\\(\\d{6}Z.*?\\)"),
           time_date = str_replace_all(time_date, "(\\()|(\\))", ""),
           message_mday = str_sub(time_date, 1L, 2L),
           message_month = str_extract(time_date, months_regex),
           message_year = str_extract(time_date, "\\s\\d{4}"),
           message_date = as.Date(paste0(message_year,
                                         message_month,
                                         message_mday),
                                  format = "%Y%b%d"),
           message_time = str_sub(time_date, 3L, 6L),
           message_time = str_replace(message_time, "(\\d{2})", "\\1:"),
           message_date_time = as.POSIXct(paste(message_date,
                                                message_time),
                                          "%Y-%m-%d %H:%M", tz = "GMT")) %>%
    rename(zulu_time_date = time_date) %>%
    select(-message_mday, -message_month, -message_year) %>%
    mutate(coord_precision = if_else(str_detect(coord_precision, "fine"),
                                     "fine", "coarse")) %>%
    select(message_date_time, relevant_terms, long, lat, 
           coord_precision, message, zulu_time_date, ID) %>%
    distinct() %>%
    arrange(desc(message_date_time))
```

# Results

```{r}
head(total_df)
glimpse(total_df)
```

# The Resulting Table

```{r}
pander::panderOptions('table.split.table', Inf)
pander::pander(head(total_df, n = 5))
```

# Save

As the format to be used is not yet determined, I save a few different versions that are easy to distribute, including an `.rda` file that is simple to load for future use and include in a package.

```{r}
readr::write_excel_csv(total_df, "data_raw/total_df_excel.csv")
readr::write_csv(total_df, "data_raw/total_df.csv")
save(total_df, file = "data/total_df.rda")
```


# `sessionInfo()`

```{r}
sessionInfo()
```




